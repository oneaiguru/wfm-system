# Complex Workflow APIs - Enterprise Business Process Management

## Overview

This module implements sophisticated enterprise-grade workflow management APIs (Tasks 66-70) with advanced business process automation, AI-powered optimization, and real-time modification capabilities.

## üéØ Enterprise Features

### Multi-Step Workflow Creation (Task 66)
- **Endpoint**: `POST /api/v1/workflows/multi-step/create`
- **Features**: Complex workflow definitions with dynamic routing, hierarchical approvals, conditional logic
- **Database Tables**: `workflow_definitions`, `step_configurations`, `routing_rules`, `approval_hierarchies`

### Conditional Routing Engine (Task 67)
- **Endpoint**: `PUT /api/v1/workflows/conditional/route`
- **Features**: Business rule engine, decision trees, data-driven routing, ML confidence scoring
- **Database Tables**: `routing_conditions`, `business_rules`, `decision_trees`, `routing_analytics`

### Parallel Execution Engine (Task 68)
- **Endpoint**: `POST /api/v1/workflows/parallel/execute`
- **Features**: Concurrent processing, synchronization points, load balancing, failure handling
- **Database Tables**: `parallel_executions`, `task_executions`, `synchronization_points`, `thread_management`

### Advanced Analytics & AI (Task 69)
- **Endpoint**: `GET /api/v1/workflows/advanced/analytics`
- **Features**: Performance analysis, bottleneck detection, ML predictions, optimization recommendations
- **Database Tables**: `workflow_performance`, `bottleneck_analysis`, `optimization_suggestions`, `analytics_results`

### Dynamic Runtime Modification (Task 70)
- **Endpoint**: `POST /api/v1/workflows/dynamic/modify`
- **Features**: Live updates, change management, rollback capabilities, impact analysis
- **Database Tables**: `runtime_modifications`, `change_requests`, `dynamic_routing`, `rollback_snapshots`

## üöÄ Quick Start

### 1. Create a Multi-Step Workflow

```python
import requests

workflow_definition = {
    "workflow_name": "Employee Onboarding Process",
    "workflow_description": "Complete employee onboarding with approvals",
    "category": "employee_request",
    "steps": [
        {
            "step_id": "initial_request",
            "step_name": "Initial Request Submission",
            "step_type": "user_task",
            "order_sequence": 1,
            "required_roles": ["employee"],
            "estimated_duration_minutes": 15
        },
        {
            "step_id": "manager_approval",
            "step_name": "Manager Approval",
            "step_type": "approval",
            "order_sequence": 2,
            "required_roles": ["manager"],
            "estimated_duration_minutes": 60,
            "auto_escalation_hours": 24
        },
        {
            "step_id": "hr_processing",
            "step_name": "HR Processing",
            "step_type": "automated",
            "order_sequence": 3,
            "required_roles": ["hr_specialist"],
            "estimated_duration_minutes": 30
        }
    ],
    "approval_hierarchies": [
        {
            "level": 1,
            "role_required": "manager",
            "minimum_level": 1
        },
        {
            "level": 2,
            "role_required": "hr_director",
            "minimum_level": 2
        }
    ],
    "routing_rules": [
        {
            "rule_id": "high_priority_route",
            "condition_expression": "priority = 'high'",
            "target_step_id": "manager_approval",
            "priority": 1
        }
    ]
}

response = requests.post(
    "http://localhost:8000/api/v1/workflows/multi-step/create",
    json=workflow_definition
)

workflow_result = response.json()
print(f"Created workflow: {workflow_result['workflow_id']}")
```

### 2. Execute Conditional Routing

```python
routing_request = {
    "workflow_id": workflow_result['workflow_id'],
    "context": {
        "workflow_instance_id": "instance_123",
        "current_step_id": "initial_request",
        "workflow_data": {
            "priority": "high",
            "amount": 5000,
            "department": "engineering"
        },
        "user_context": {
            "user_id": "emp_456",
            "role": "employee",
            "department": "engineering"
        }
    },
    "business_rules": [
        {
            "rule_id": "amount_threshold",
            "rule_name": "High Amount Approval",
            "condition_type": "data_threshold",
            "condition_expression": "amount > 1000",
            "target_action": "route_to_step",
            "target_step_id": "manager_approval",
            "priority": 10
        }
    ]
}

response = requests.put(
    "http://localhost:8000/api/v1/workflows/conditional/route",
    json=routing_request
)

routing_result = response.json()
print(f"Routing decision: {routing_result['routing_decision']['routing_action']}")
```

### 3. Execute Parallel Tasks

```python
parallel_request = {
    "workflow_id": workflow_result['workflow_id'],
    "workflow_instance_id": "instance_123",
    "tasks": [
        {
            "task_id": "background_check",
            "task_name": "Background Check",
            "task_type": "integration",
            "step_id": "hr_processing",
            "priority": 100,
            "estimated_duration_minutes": 45,
            "dependencies": []
        },
        {
            "task_id": "equipment_setup",
            "task_name": "Equipment Setup",
            "task_type": "automation",
            "step_id": "hr_processing", 
            "priority": 90,
            "estimated_duration_minutes": 30,
            "dependencies": []
        },
        {
            "task_id": "access_provisioning",
            "task_name": "Access Provisioning",
            "task_type": "automation",
            "step_id": "hr_processing",
            "priority": 80,
            "estimated_duration_minutes": 20,
            "dependencies": ["background_check"]
        }
    ],
    "synchronization_points": [
        {
            "sync_id": "hr_completion",
            "sync_name": "HR Processing Complete",
            "sync_type": "barrier",
            "required_tasks": ["background_check", "equipment_setup", "access_provisioning"],
            "minimum_completion_percentage": 100
        }
    ],
    "load_balancing": {
        "strategy": "priority_based",
        "max_concurrent_tasks": 3
    }
}

response = requests.post(
    "http://localhost:8000/api/v1/workflows/parallel/execute",
    json=parallel_request
)

execution_result = response.json()
print(f"Execution started: {execution_result['execution_id']}")
```

### 4. Get Advanced Analytics

```python
response = requests.get(
    "http://localhost:8000/api/v1/workflows/advanced/analytics",
    params={
        "workflow_ids": workflow_result['workflow_id'],
        "date_range_days": 30,
        "include_predictions": True,
        "include_recommendations": True,
        "analysis_depth": "comprehensive"
    }
)

analytics_result = response.json()

print("Performance Metrics:")
for metric in analytics_result['performance_metrics']:
    print(f"  {metric['workflow_name']}: {metric['avg_execution_time_minutes']} min avg, {metric['success_rate_percentage']}% success")

print("\\nOptimization Recommendations:")
for rec in analytics_result['optimization_recommendations']:
    print(f"  {rec['title']}: {rec['estimated_impact_percentage']}% improvement, ROI: {rec['estimated_roi']}")

print("\\nAI Insights:")
for insight in analytics_result['ai_insights']:
    print(f"  ‚Ä¢ {insight}")
```

### 5. Dynamic Workflow Modification

```python
modification_request = {
    "workflow_modification": {
        "workflow_id": workflow_result['workflow_id'],
        "modification_type": "add_step",
        "step_modifications": [
            {
                "step_id": "document_upload",
                "modification_type": "add_step",
                "insert_after_step": "initial_request",
                "new_configuration": {
                    "step_name": "Document Upload",
                    "step_type": "user_task",
                    "required_roles": ["employee"],
                    "estimated_duration_minutes": 10,
                    "form_schema": {
                        "fields": [
                            {"name": "resume", "type": "file", "required": true},
                            {"name": "references", "type": "file", "required": false}
                        ]
                    }
                }
            }
        ],
        "reason": "Adding document upload step for compliance requirements",
        "requested_by": "system_admin",
        "emergency_change": False
    },
    "impact_analysis_required": True,
    "auto_apply": False,
    "rollback_strategy": "graceful",
    "monitoring_duration_minutes": 60
}

response = requests.post(
    "http://localhost:8000/api/v1/workflows/dynamic/modify",
    json=modification_request
)

modification_result = response.json()
print(f"Modification status: {modification_result['modification_result']['status']}")
print(f"Impact level: {modification_result['modification_result']['impact_analysis']['impact_level']}")
```

## üìä Database Schema

### Core Workflow Tables

```sql
-- Main workflow definitions
CREATE TABLE workflow_definitions (
    workflow_id VARCHAR(36) PRIMARY KEY,
    workflow_name VARCHAR(100) NOT NULL,
    workflow_description TEXT,
    category VARCHAR(50) NOT NULL,
    version VARCHAR(20) DEFAULT '1.0',
    is_active BOOLEAN DEFAULT true,
    global_timeout_hours INTEGER DEFAULT 168,
    retry_configuration JSONB DEFAULT '{}',
    notification_escalation JSONB DEFAULT '{}',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Individual step configurations
CREATE TABLE step_configurations (
    step_config_id VARCHAR(36) PRIMARY KEY,
    workflow_id VARCHAR(36) NOT NULL,
    step_id VARCHAR(100) NOT NULL,
    step_name VARCHAR(100) NOT NULL,
    step_type VARCHAR(50) NOT NULL,
    order_sequence INTEGER NOT NULL,
    required_roles JSONB DEFAULT '[]',
    estimated_duration_minutes INTEGER DEFAULT 30,
    auto_escalation_hours INTEGER NULL,
    conditions JSONB DEFAULT '{}',
    form_schema JSONB DEFAULT '{}',
    notification_settings JSONB DEFAULT '{}',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (workflow_id) REFERENCES workflow_definitions(workflow_id)
);

-- Dynamic routing rules
CREATE TABLE routing_rules (
    rule_id VARCHAR(100) PRIMARY KEY,
    workflow_id VARCHAR(36) NOT NULL,
    condition_expression TEXT NOT NULL,
    target_step_id VARCHAR(100) NOT NULL,
    priority INTEGER DEFAULT 100,
    description TEXT,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (workflow_id) REFERENCES workflow_definitions(workflow_id)
);
```

### Execution & Analytics Tables

```sql
-- Parallel execution tracking
CREATE TABLE parallel_executions (
    execution_id VARCHAR(36) PRIMARY KEY,
    workflow_id VARCHAR(36) NOT NULL,
    workflow_instance_id VARCHAR(36) NOT NULL,
    total_tasks INTEGER NOT NULL,
    tasks_completed INTEGER DEFAULT 0,
    tasks_failed INTEGER DEFAULT 0,
    tasks_running INTEGER DEFAULT 0,
    overall_status VARCHAR(50) DEFAULT 'initializing',
    execution_progress_percentage DECIMAL(5,2) DEFAULT 0.0,
    start_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    estimated_completion_time TIMESTAMP,
    performance_metrics JSONB DEFAULT '{}'
);

-- Performance analytics
CREATE TABLE workflow_performance (
    performance_id VARCHAR(36) PRIMARY KEY,
    workflow_id VARCHAR(36) NOT NULL,
    execution_date DATE NOT NULL,
    total_executions INTEGER DEFAULT 0,
    successful_executions INTEGER DEFAULT 0,
    avg_execution_time_minutes DECIMAL(10,2) DEFAULT 0.0,
    resource_utilization JSONB DEFAULT '{}'
);

-- Runtime modifications
CREATE TABLE runtime_modifications (
    modification_id VARCHAR(36) PRIMARY KEY,
    workflow_id VARCHAR(36) NOT NULL,
    modification_type VARCHAR(50) NOT NULL,
    status VARCHAR(50) DEFAULT 'pending',
    requested_by VARCHAR(100) NOT NULL,
    reason TEXT NOT NULL,
    modification_details JSONB NOT NULL,
    impact_analysis JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## üîß Configuration

### Environment Variables

```bash
# Database Configuration
POSTGRES_SERVER=localhost
POSTGRES_USER=wfm_user
POSTGRES_PASSWORD=wfm_password
POSTGRES_DB=wfm_enterprise

# Workflow Settings
MAX_CONCURRENT_EXECUTIONS=50
DEFAULT_TIMEOUT_HOURS=168
MONITORING_INTERVAL_SECONDS=30

# Analytics Settings
ML_PREDICTION_ENABLED=true
OPTIMIZATION_RECOMMENDATIONS_ENABLED=true
TREND_ANALYSIS_DAYS=30

# Modification Settings
EMERGENCY_MODIFICATION_ENABLED=true
AUTO_ROLLBACK_ENABLED=true
APPROVAL_REQUIRED_FOR_HIGH_IMPACT=true
```

## üîê Security Features

- **Role-Based Access Control**: Multi-level approval hierarchies
- **Audit Trails**: Complete modification and execution history
- **Change Management**: Approval workflows for high-impact changes
- **Emergency Procedures**: Emergency modification with override capabilities
- **Data Validation**: Comprehensive input validation and sanitization

## üìà Performance Features

- **Async Processing**: Non-blocking execution with background tasks
- **Load Balancing**: Intelligent task distribution with resource awareness
- **Caching**: Performance metrics caching with TTL
- **Connection Pooling**: Optimized database connection management
- **Monitoring**: Real-time performance metrics and alerting

## üö® Error Handling

- **Graceful Degradation**: Partial success handling for parallel executions
- **Automatic Retry**: Configurable retry strategies with exponential backoff
- **Circuit Breaker**: Failure detection and automatic service protection
- **Rollback Capabilities**: Point-in-time restoration for failed modifications
- **Health Checks**: Comprehensive system health monitoring

## üìö API Documentation

All endpoints include comprehensive OpenAPI documentation with:
- Detailed parameter descriptions
- Request/response schemas
- Example requests and responses
- Error codes and messages
- Performance considerations

## üéØ Enterprise Integration

### Supported Integrations
- **1C ERP Systems**: Employee data synchronization
- **Contact Centers**: Real-time workflow triggers
- **External APIs**: REST/GraphQL integration points
- **Message Queues**: Async workflow triggering
- **Notification Systems**: Multi-channel notifications

### Monitoring & Observability
- **Metrics**: Prometheus-compatible metrics export
- **Logging**: Structured JSON logging with correlation IDs
- **Tracing**: Distributed tracing for complex workflows
- **Dashboards**: Real-time executive and operational dashboards
- **Alerting**: Intelligent alerting with escalation chains

## üîÑ Deployment

### Production Deployment
```bash
# Docker deployment
docker build -t wfm-complex-workflows .
docker run -p 8000:8000 wfm-complex-workflows

# Kubernetes deployment
kubectl apply -f k8s/complex-workflows-deployment.yaml
kubectl apply -f k8s/complex-workflows-service.yaml

# Database migration
alembic upgrade head
```

### Scaling Considerations
- Horizontal pod autoscaling for high-volume periods
- Database read replicas for analytics queries
- Redis cluster for session and cache management
- Load balancer configuration for API endpoints

## üìû Support

For enterprise support and implementation assistance:
- Documentation: `/api/v1/workflows/capabilities`
- Health Check: `/api/v1/workflows/health`
- System Metrics: `/api/v1/workflows/system/metrics`
- Technical Support: Enterprise support portal