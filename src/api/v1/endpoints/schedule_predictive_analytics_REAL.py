"""
REAL SCHEDULE PREDICTIVE ANALYTICS ENDPOINT
Task 47/50: Predictive Analytics and Forecasting for Schedule Planning
"""

from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
from pydantic import BaseModel
from datetime import datetime, date, timedelta
from typing import Optional, List, Dict, Any
from uuid import UUID
import uuid
import json
import statistics

from ...core.database import get_db

router = APIRouter()

class PredictiveAnalyticsRequest(BaseModel):
    prediction_scope: str = "–æ—Ç–¥–µ–ª"
    scope_id: UUID
    prediction_horizon_days: int = 30
    analysis_models: List[str] = ["–∑–∞–≥—Ä—É–∑–∫–∞", "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã", "–∑–∞—Ç—Ä–∞—Ç—ã"]
    historical_period_days: int = 90

class PredictiveAnalyticsResponse(BaseModel):
    prediction_id: str
    predictions: Dict[str, Any]
    confidence_levels: Dict[str, float]
    trend_analysis: Dict[str, Any]
    recommendations: List[str]
    message: str

@router.post("/schedules/analytics/predictive", response_model=PredictiveAnalyticsResponse, tags=["üî• REAL Schedule Analytics"])
async def generate_predictive_analytics(
    request: PredictiveAnalyticsRequest,
    db: AsyncSession = Depends(get_db)
):
    """REAL PREDICTIVE ANALYTICS - NO MOCKS! Generates predictions based on historical data"""
    try:
        # Define analysis periods
        today = date.today()
        historical_start = today - timedelta(days=request.historical_period_days)
        prediction_end = today + timedelta(days=request.prediction_horizon_days)
        
        # Get historical data for analysis
        historical_query = text("""
            SELECT 
                ws.id, ws.total_hours, ws.optimization_score, ws.status,
                ws.created_at, ws.effective_date, ws.expiry_date,
                e.id as employee_id, e.max_hours_per_week,
                st.cost_per_hour, os.department_name,
                DATE_PART('week', ws.created_at) as week_number,
                DATE_PART('month', ws.created_at) as month_number
            FROM work_schedules_core ws
            JOIN employees e ON ws.employee_id = e.id
            JOIN organizational_structure os ON e.department_id = os.id
            LEFT JOIN schedule_templates st ON ws.template_id = st.id
            WHERE e.department_id = :scope_id
            AND ws.created_at >= :historical_start
            AND ws.created_at <= :today
            ORDER BY ws.created_at
        """)
        
        historical_result = await db.execute(historical_query, {
            "scope_id": request.scope_id,
            "historical_start": historical_start,
            "today": today
        })
        
        historical_data = historical_result.fetchall()
        
        if not historical_data:
            raise HTTPException(
                status_code=404,
                detail="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"
            )
        
        # Initialize predictions structure
        predictions = {}
        confidence_levels = {}
        
        # Workload prediction
        if "–∑–∞–≥—Ä—É–∑–∫–∞" in request.analysis_models:
            # Analyze historical workload patterns
            weekly_hours = {}
            weekly_schedule_counts = {}
            
            for record in historical_data:
                week_key = f"{record.created_at.year}-W{int(record.week_number)}"
                
                if week_key not in weekly_hours:
                    weekly_hours[week_key] = 0
                    weekly_schedule_counts[week_key] = 0
                
                weekly_hours[week_key] += record.total_hours or 0
                weekly_schedule_counts[week_key] += 1
            
            # Calculate trends
            recent_weeks = sorted(weekly_hours.keys())[-4:]  # Last 4 weeks
            if len(recent_weeks) >= 2:
                hours_values = [weekly_hours[week] for week in recent_weeks]
                schedule_counts = [weekly_schedule_counts[week] for week in recent_weeks]
                
                # Simple linear trend calculation
                hours_trend = (hours_values[-1] - hours_values[0]) / len(hours_values)
                schedule_trend = (schedule_counts[-1] - schedule_counts[0]) / len(schedule_counts)
                
                # Predict future workload
                current_avg_hours = statistics.mean(hours_values)
                current_avg_schedules = statistics.mean(schedule_counts)
                
                future_weeks = request.prediction_horizon_days // 7
                predicted_hours = current_avg_hours + (hours_trend * future_weeks)
                predicted_schedules = current_avg_schedules + (schedule_trend * future_weeks)
                
                predictions["–∑–∞–≥—Ä—É–∑–∫–∞"] = {
                    "–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ_—á–∞—Å—ã_–Ω–µ–¥–µ–ª—è": round(max(0, predicted_hours), 1),
                    "–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ_—Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è_–Ω–µ–¥–µ–ª—è": round(max(0, predicted_schedules), 1),
                    "—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è_—á–∞—Å–æ–≤": "—Ä–æ—Å—Ç" if hours_trend > 0 else "—Å–Ω–∏–∂–µ–Ω–∏–µ" if hours_trend < 0 else "—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å",
                    "—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è_—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–π": "—Ä–æ—Å—Ç" if schedule_trend > 0 else "—Å–Ω–∏–∂–µ–Ω–∏–µ" if schedule_trend < 0 else "—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å",
                    "–∏–∑–º–µ–Ω–µ–Ω–∏–µ_—á–∞—Å–æ–≤_–≤_–Ω–µ–¥–µ–ª—é": round(hours_trend, 1),
                    "—Ç–µ–∫—É—â–∞—è_—Å—Ä–µ–¥–Ω—è—è_–Ω–µ–¥–µ–ª—è": round(current_avg_hours, 1)
                }
                
                # Confidence based on data consistency
                hours_variance = statistics.variance(hours_values) if len(hours_values) > 1 else 0
                confidence_levels["–∑–∞–≥—Ä—É–∑–∫–∞"] = max(0.5, min(0.95, 1 - (hours_variance / (current_avg_hours + 1)) * 0.5))
        
        # Efficiency prediction
        if "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å" in request.analysis_models:
            optimization_scores = [r.optimization_score for r in historical_data if r.optimization_score]
            
            if optimization_scores:
                # Group by time periods to see trends
                monthly_scores = {}
                for record in historical_data:
                    if record.optimization_score:
                        month_key = f"{record.created_at.year}-{int(record.month_number):02d}"
                        if month_key not in monthly_scores:
                            monthly_scores[month_key] = []
                        monthly_scores[month_key].append(record.optimization_score)
                
                # Calculate monthly averages
                monthly_averages = {month: statistics.mean(scores) for month, scores in monthly_scores.items()}
                recent_months = sorted(monthly_averages.keys())[-3:]  # Last 3 months
                
                if len(recent_months) >= 2:
                    score_values = [monthly_averages[month] for month in recent_months]
                    efficiency_trend = (score_values[-1] - score_values[0]) / len(score_values)
                    current_efficiency = statistics.mean(score_values)
                    
                    predicted_efficiency = current_efficiency + efficiency_trend
                    
                    predictions["—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"] = {
                        "–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π_–±–∞–ª–ª_–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏": round(max(0, min(100, predicted_efficiency)), 2),
                        "—Ç–µ–∫—É—â–∏–π_—Å—Ä–µ–¥–Ω–∏–π_–±–∞–ª–ª": round(current_efficiency, 2),
                        "—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è": "—É–ª—É—á—à–µ–Ω–∏–µ" if efficiency_trend > 0 else "—É—Ö—É–¥—à–µ–Ω–∏–µ" if efficiency_trend < 0 else "—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å",
                        "–∏–∑–º–µ–Ω–µ–Ω–∏–µ_–≤_–º–µ—Å—è—Ü": round(efficiency_trend, 2),
                        "–ø—Ä–æ–≥–Ω–æ–∑_–∫–∞—Ç–µ–≥–æ—Ä–∏–∏": "–æ—Ç–ª–∏—á–Ω–æ" if predicted_efficiency >= 80 else "—Ö–æ—Ä–æ—à–æ" if predicted_efficiency >= 60 else "—Ç—Ä–µ–±—É–µ—Ç_—É–ª—É—á—à–µ–Ω–∏—è"
                    }
                    
                    score_variance = statistics.variance(score_values) if len(score_values) > 1 else 0
                    confidence_levels["—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"] = max(0.6, min(0.9, 1 - (score_variance / 100)))
        
        # Conflict prediction
        if "–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã" in request.analysis_models:
            # Get conflict history
            conflicts_query = text("""
                SELECT 
                    cd.created_at, cd.conflict_summary,
                    DATE_PART('week', cd.created_at) as week_number
                FROM conflict_detections cd
                WHERE cd.scope_id = :scope_id
                AND cd.created_at >= :historical_start
                ORDER BY cd.created_at
            """)
            
            conflicts_result = await db.execute(conflicts_query, {
                "scope_id": request.scope_id,
                "historical_start": historical_start
            })
            
            conflicts_data = conflicts_result.fetchall()
            
            # Count conflicts per week
            weekly_conflicts = {}
            for conflict in conflicts_data:
                week_key = f"{conflict.created_at.year}-W{int(conflict.week_number)}"
                if week_key not in weekly_conflicts:
                    weekly_conflicts[week_key] = 0
                
                # Parse conflict summary to count actual conflicts
                if conflict.conflict_summary:
                    summary = json.loads(conflict.conflict_summary)
                    weekly_conflicts[week_key] += summary.get("–≤—Å–µ–≥–æ_–∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤", 1)
                else:
                    weekly_conflicts[week_key] += 1
            
            if weekly_conflicts:
                recent_conflict_weeks = sorted(weekly_conflicts.keys())[-4:]
                conflict_values = [weekly_conflicts.get(week, 0) for week in recent_conflict_weeks]
                
                avg_conflicts = statistics.mean(conflict_values)
                conflict_trend = (conflict_values[-1] - conflict_values[0]) / len(conflict_values) if len(conflict_values) > 1 else 0
                
                future_weeks = request.prediction_horizon_days // 7
                predicted_conflicts = max(0, avg_conflicts + (conflict_trend * future_weeks))
                
                predictions["–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã"] = {
                    "–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ_–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã_–Ω–µ–¥–µ–ª—è": round(predicted_conflicts, 1),
                    "—Ç–µ–∫—É—â–∞—è_—Å—Ä–µ–¥–Ω—è—è_–Ω–µ–¥–µ–ª—è": round(avg_conflicts, 1),
                    "—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è": "—Ä–æ—Å—Ç" if conflict_trend > 0 else "—Å–Ω–∏–∂–µ–Ω–∏–µ" if conflict_trend < 0 else "—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å",
                    "–∏–∑–º–µ–Ω–µ–Ω–∏–µ_–≤_–Ω–µ–¥–µ–ª—é": round(conflict_trend, 1),
                    "—Ä–∏—Å–∫_—É—Ä–æ–≤–µ–Ω—å": "–≤—ã—Å–æ–∫–∏–π" if predicted_conflicts > 5 else "—Å—Ä–µ–¥–Ω–∏–π" if predicted_conflicts > 2 else "–Ω–∏–∑–∫–∏–π"
                }
                
                conflict_variance = statistics.variance(conflict_values) if len(conflict_values) > 1 else 0
                confidence_levels["–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã"] = max(0.5, min(0.85, 1 - (conflict_variance / (avg_conflicts + 1)) * 0.3))
        
        # Cost prediction
        if "–∑–∞—Ç—Ä–∞—Ç—ã" in request.analysis_models:
            # Calculate historical costs
            weekly_costs = {}
            for record in historical_data:
                week_key = f"{record.created_at.year}-W{int(record.week_number)}"
                if week_key not in weekly_costs:
                    weekly_costs[week_key] = 0
                
                cost_per_hour = record.cost_per_hour or 1000
                weekly_costs[week_key] += (record.total_hours or 0) * cost_per_hour
            
            if weekly_costs:
                recent_weeks = sorted(weekly_costs.keys())[-4:]
                cost_values = [weekly_costs[week] for week in recent_weeks]
                
                avg_cost = statistics.mean(cost_values)
                cost_trend = (cost_values[-1] - cost_values[0]) / len(cost_values) if len(cost_values) > 1 else 0
                
                future_weeks = request.prediction_horizon_days // 7
                predicted_weekly_cost = avg_cost + (cost_trend * future_weeks)
                predicted_monthly_cost = predicted_weekly_cost * 4.33  # Average weeks per month
                
                predictions["–∑–∞—Ç—Ä–∞—Ç—ã"] = {
                    "–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ_–∑–∞—Ç—Ä–∞—Ç—ã_–Ω–µ–¥–µ–ª—è": round(max(0, predicted_weekly_cost), 2),
                    "–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ_–∑–∞—Ç—Ä–∞—Ç—ã_–º–µ—Å—è—Ü": round(max(0, predicted_monthly_cost), 2),
                    "—Ç–µ–∫—É—â–∞—è_—Å—Ä–µ–¥–Ω—è—è_–Ω–µ–¥–µ–ª—è": round(avg_cost, 2),
                    "—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è": "—Ä–æ—Å—Ç" if cost_trend > 0 else "—Å–Ω–∏–∂–µ–Ω–∏–µ" if cost_trend < 0 else "—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å",
                    "–∏–∑–º–µ–Ω–µ–Ω–∏–µ_–≤_–Ω–µ–¥–µ–ª—é": round(cost_trend, 2),
                    "–±—é–¥–∂–µ—Ç–Ω—ã–π_—Å—Ç–∞—Ç—É—Å": "–ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ" if predicted_monthly_cost > avg_cost * 4.33 * 1.1 else "–≤_—Ä–∞–º–∫–∞—Ö"
                }
                
                cost_variance = statistics.variance(cost_values) if len(cost_values) > 1 else 0
                confidence_levels["–∑–∞—Ç—Ä–∞—Ç—ã"] = max(0.6, min(0.9, 1 - (cost_variance / (avg_cost + 1)) * 0.0001))
        
        # Trend analysis summary
        trend_analysis = {
            "–æ–±—â–∏–µ_—Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏": [],
            "–ø–µ—Ä–∏–æ–¥_–∞–Ω–∞–ª–∏–∑–∞": f"{historical_start} - {today}",
            "–≥–æ—Ä–∏–∑–æ–Ω—Ç_–ø—Ä–æ–≥–Ω–æ–∑–∞": f"{today} - {prediction_end}",
            "–∫–∞—á–µ—Å—Ç–≤–æ_–¥–∞–Ω–Ω—ã—Ö": "—Ö–æ—Ä–æ—à–µ–µ" if len(historical_data) > 20 else "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ",
            "—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ_–¥–µ–π—Å—Ç–≤–∏—è": []
        }
        
        # Analyze overall trends
        for model, prediction in predictions.items():
            if "—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è" in prediction:
                trend_analysis["–æ–±—â–∏–µ_—Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏"].append(f"{model}: {prediction['—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è']}")
        
        # Generate recommendations
        recommendations = []
        
        if "–∑–∞–≥—Ä—É–∑–∫–∞" in predictions and predictions["–∑–∞–≥—Ä—É–∑–∫–∞"]["—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è"] == "—Ä–æ—Å—Ç":
            recommendations.append("–û–∂–∏–¥–∞–µ—Ç—Å—è —Ä–æ—Å—Ç –Ω–∞–≥—Ä—É–∑–∫–∏ - –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã")
        
        if "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å" in predictions and predictions["—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"]["–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π_–±–∞–ª–ª_–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"] < 70:
            recommendations.append("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è —Å–Ω–∏–∂–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ - –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∏—Ç–µ —à–∞–±–ª–æ–Ω—ã —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–π")
        
        if "–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã" in predictions and predictions["–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã"]["—Ä–∏—Å–∫_—É—Ä–æ–≤–µ–Ω—å"] == "–≤—ã—Å–æ–∫–∏–π":
            recommendations.append("–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ - —É—Å–∏–ª—å—Ç–µ –∫–æ–Ω—Ç—Ä–æ–ª—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        if "–∑–∞—Ç—Ä–∞—Ç—ã" in predictions and predictions["–∑–∞—Ç—Ä–∞—Ç—ã"]["—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è"] == "—Ä–æ—Å—Ç":
            recommendations.append("–û–∂–∏–¥–∞–µ—Ç—Å—è —Ä–æ—Å—Ç –∑–∞—Ç—Ä–∞—Ç - –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤")
        
        if not recommendations:
            recommendations.append("–ü—Ä–æ–≥–Ω–æ–∑—ã —Å—Ç–∞–±–∏–ª—å–Ω—ã - –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")
        
        # Store prediction record
        prediction_id = str(uuid.uuid4())
        current_time = datetime.utcnow()
        
        prediction_record_query = text("""
            INSERT INTO predictive_analytics 
            (id, prediction_scope, scope_id, prediction_horizon_days,
             predictions, confidence_levels, trend_analysis, created_at)
            VALUES 
            (:id, :scope, :scope_id, :horizon,
             :predictions, :confidence, :trends, :created_at)
        """)
        
        await db.execute(prediction_record_query, {
            'id': prediction_id,
            'scope': request.prediction_scope,
            'scope_id': request.scope_id,
            'horizon': request.prediction_horizon_days,
            'predictions': json.dumps(predictions),
            'confidence': json.dumps(confidence_levels),
            'trends': json.dumps(trend_analysis),
            'created_at': current_time
        })
        
        await db.commit()
        
        # Calculate average confidence
        avg_confidence = statistics.mean(confidence_levels.values()) if confidence_levels else 0.7
        
        return PredictiveAnalyticsResponse(
            prediction_id=prediction_id,
            predictions=predictions,
            confidence_levels=confidence_levels,
            trend_analysis=trend_analysis,
            recommendations=recommendations,
            message=f"–ü—Ä–æ–≥–Ω–æ–∑–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ {request.prediction_horizon_days} –¥–Ω–µ–π. –°—Ä–µ–¥–Ω—è—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: {avg_confidence:.1%}. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(request.analysis_models)} –º–æ–¥–µ–ª–µ–π"
        )
        
    except Exception as e:
        await db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {str(e)}"
        )